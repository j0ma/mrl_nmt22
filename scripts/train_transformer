#!/bin/bash
# Sweeps over data and hyperparameters.

set -euo pipefail

# Defaults.
readonly SEED=$1
readonly CRITERION=$2
readonly LABEL_SMOOTHING=$3
readonly OPTIMIZER=$4
readonly LR=$5
readonly LR_SCHEDULER=$6
readonly WARMUP_INIT_LR=$7
readonly WARMUP_UPDATES=$8

WARMUP_UPDATES_FLAG="--warmup-updates=${WARMUP_UPDATES}"

if [[ "${LR_SCHEDULER}" == "inverse_sqrt" ]]; then
    WARMUP_INIT_LR_FLAG="--warmup-init-lr=${WARMUP_INIT_LR}"
else
    WARMUP_INIT_LR_FLAG=""
fi

echo "warmup init LR flag: ${WARMUP_INIT_LR_FLAG}"

readonly CLIP_NORM=$9
readonly MAX_UPDATE=${10}
readonly SAVE_INTERVAL=${11}
readonly ENCODER_LAYERS=${12}
readonly ENCODER_ATTENTION_HEADS=${13}
readonly DECODER_LAYERS=${14}
readonly DECODER_ATTENTION_HEADS=${15}
readonly ACTIVATION_FN=${16}

# Hyperparameters to be tuned.
readonly BATCH_SIZE=${17}
readonly P_DROPOUT=${18}

# Encoder / decoder sizes
readonly DED=${19}
readonly DHS=${20}
readonly EED=${21}
readonly EHS=${22}

# Path to binarized data & checkpoints
readonly EXPERIMENT_NAME=${23}

# GPU device
readonly GPU_DEVICE="${24}"

# Validate interval (note: these are new in fairseq 0.10.0)
#echo "validate interval: ${25}"
readonly VALIDATE_INTERVAL=${25}
readonly VALIDATE_INTERVAL_UPDATES=${26}

readonly SRC_LANG=${27}
readonly TGT_LANG=${28}

readonly MAX_EPOCH=${29}

readonly MODEL_NAME="${30}"

# Derive some paths based on the above
readonly EXPERIMENT_FOLDER="$(pwd)/experiments/${EXPERIMENT_NAME}"
readonly DATA_BIN_PATH="${EXPERIMENT_FOLDER}/binarized_data"
readonly CHECKPOINT_FOLDER="${EXPERIMENT_FOLDER}/${MODEL_NAME}/checkpoints"

get_max_flag() {
    if [ "${MAX_UPDATE}" = "0" ]
    then
        max_flag="--max-epoch=${MAX_EPOCH}"
    else
        max_flag="--max-update=${MAX_UPDATE}"
    fi
    echo "${max_flag}"
}
MAX_UPDATE_FLAG=$(get_max_flag)

train() {
    local -r CP="$1"
    shift
    
    CUDA_VISIBLE_DEVICES=${GPU_DEVICE} fairseq-train \
        "${DATA_BIN_PATH}" \
        --save-dir="${CP}" \
        --source-lang="${SRC_LANG}" \
        --target-lang="${TGT_LANG}" \
        --seed="${SEED}" \
        --arch=transformer \
        --attention-dropout="${P_DROPOUT}" \
        --activation-dropout="${P_DROPOUT}" \
        --activation-fn="${ACTIVATION_FN}" \
        --encoder-embed-dim="${EED}" \
        --encoder-ffn-embed-dim="${EHS}" \
        --encoder-layers="${ENCODER_LAYERS}" \
        --encoder-attention-heads="${ENCODER_ATTENTION_HEADS}" \
        --encoder-normalize-before \
        --decoder-embed-dim="${DED}" \
        --decoder-ffn-embed-dim="${DHS}" \
        --decoder-layers="${DECODER_LAYERS}" \
        --decoder-attention-heads="${DECODER_ATTENTION_HEADS}" \
        --decoder-normalize-before \
        --share-decoder-input-output-embed \
        --criterion="${CRITERION}" \
        --label-smoothing="${LABEL_SMOOTHING}" \
        --optimizer="${OPTIMIZER}" \
        --lr="${LR}" \
        --lr-scheduler="${LR_SCHEDULER}" \
        --clip-norm="${CLIP_NORM}" \
        --batch-size="${BATCH_SIZE}" \
        --save-interval="${SAVE_INTERVAL}" \
        --validate-interval-updates="${VALIDATE_INTERVAL_UPDATES}" \
        --fp16 \
        ${MAX_UPDATE_FLAG} ${WARMUP_UPDATES_FLAG} ${WARMUP_INIT_LR_FLAG}

}

# These set the encoder and/or decoder size.

echo "Encoder embedding dim: ${EED}"
echo "Encoder hidden size: ${EHS}"
echo "Decoder embedding dim: ${DED}"
echo "Decoder hidden size: ${DHS}"

echo "${CHECKPOINT_FOLDER}" | tee --append "${EXPERIMENT_FOLDER}/train_log.log"
train "${CHECKPOINT_FOLDER}" | tee --append "${EXPERIMENT_FOLDER}/train_log.log"
