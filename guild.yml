# Configs
- config: basic-flags
  flags:
    experiment_name:
      type: string
      required: true
    src_lang:
      type: string
      required: true
    tgt_lang:
      type: string
      required: true
    model_name:
      type: string
      default: "transformer"
    seed:
      type: int
      default: 1917
    gpu_device:
      type: string
      default: 1
- config: prep-flags
  flags:
    raw_data_folder:
      type: string
      required: true
    bin_data_folder:
      type: string
      required: true
- config: evaluate-flags
  flags:
    mode:
      type: string
      default: "dev"
    beam_size:
      type: int
      default: 5
    remove_preprocessing_source:
      type: string
      default: "none"
    remove_preprocessing_hypotheses:
      type: string
      default: "none"
    remove_preprocessing_references:
      type: string
      default: "none"
    remove_preprocessing_references_clean:
      type: string
      default: "none"
    detokenize_source:
      type: string
      default: true
    detokenize_hypotheses:
      type: string
      default: true
    detokenize_references:
      type: string
      default: true
    detokenize_references_clean:
      type: string
      default: false
    references_clean_file:
      type: string
      default: ""
    use_cpu_for_eval:
      type: string
      default: ""
- config: transformer-flags
  flags:
    criterion:
      type: string
      default: "label_smoothed_cross_entropy"
    label_smoothing:
      type: float
      default: 0.1
    lr:
      type: float
      default: 0.0003
    lr_scheduler:
      type: string
      default: "fixed"
    warmup_init_lr:
      type: float
      default: 0.00000001
    clip_norm:
      type: float
      default: 1.0
    save_interval_updates:
      type: int
      default: 25000
    activation_fn:
      type: string
      default: "relu"
    encoder_layers:
      type: int
      default: 4
    decoder_layers:
      type: int
      default: 4
    encoder_attention_heads:
      type: int
      default: 8
    decoder_attention_heads:
      type: int
      default: 8
    decoder_embedding_dim:
      type: int
      default: 200
    decoder_hidden_size:
      type: int
      default: 1024
    encoder_embedding_dim:
      type: int
      default: 200
    encoder_hidden_size:
      type: int
      default: 1024
    batch_size:
      type: int
      default: 64
    max_tokens:
      type: int
      default: 0
    optimizer:
      type: string
      default: "adadelta"
    p_dropout:
      type: float
      default: 0.2
    max_updates:
      type: int
      default: 0
    max_epoch:
      type: int
      default: 5
    validate_interval:
      type: int
      default: 1
    validate_interval_updates:
      type: int
      default: 5000
    warmup_updates:
      type: int
      default: 10000

# Data download ops
- model: data
  operations:
    download_all:
      description: "Download all data for all languages"
      flags:
        download_folder:
          type: string
          required: true
      steps:
        - run: "download_til corpus_name=default-train source_language=en target_language=tr download_folder=${download_folder}/${target_language}"
        - run: "download_til corpus_name=default-train source_language=en target_language=uz download_folder=${download_folder}/${target_language}"
        - run: "download_mtdata corpus_name=default-train source_language=en target_language=cs download_folder=${download_folder}/${target_language}"
        - run: "download_mtdata corpus_name=default-train source_language=en target_language=de download_folder=${download_folder}/${target_language}"
        - run: "download_mtdata corpus_name=default-train source_language=en target_language=ru download_folder=${download_folder}/${target_language}"
        - run: "download_mtdata corpus_name=default-train source_language=en target_language=fi download_folder=${download_folder}/${target_language}"
        - run: "download_mtdata corpus_name=default-train source_language=en target_language=et download_folder=${download_folder}/${target_language}"
        - run: "download_mtdata corpus_name=default-train source_language=en target_language=iu download_folder=${download_folder}/${target_language}"
        - run: "download_mtdata corpus_name=default-train source_language=en target_language=vi download_folder=${download_folder}/${target_language}"
      requires:
        - file: scripts
        - file: config

    download_mtdata:
      description: "Download corpora using MTData"
      exec: "bash scripts/download/download_mtdata.sh ${corpus_name} ${source_language} ${target_language} ${download_folder}"
      flags:
        corpus_name:
          type: string
          required: true
        download_folder:
          type: string
          required: true
        source_language:
          type: string
          required: true
        target_language:
          type: string
          required: true
      requires:
        - file: scripts
        - file: config
    download_til:
      description: "Download Turkic Interlingua corpus"
      main: scripts.download.download_til
      flags:
        corpus_name:
          type: string
          required: true
        download_folder:
          type: string
          required: true
        source_language:
          type: string
          required: true
        target_language:
          type: string
          required: true
        split:
          type: string
          required: true
          default: "all"
          choices:
            - train
            - dev
            - test
            - all
      requires:
        - file: scripts
        - file: config

# Slurm ops
- model: slurm
  operations:
    echo_something:
      description: "TEST"
      exec: "echo ${msg}"
      flags:
        msg:
          required: true
    hardcoded_en_char_uz_char_til:
      description: "Submit a SLURM job to train a EN-UZ char-level model (hardcoded debug)"
      exec: "sbatch --gres=gpu:V100:1 scripts/scratch/slurm_hardcoded_en_char_uz_char_til.sh"
      requires:
        - file: scripts
        - file: config
        - file: data-bin
        - file: checkpoints
        - file: experiments
    en_char_uz_char_til:
      description: "Submit a SLURM job to train a EN-UZ char-level model"
      exec: "sbatch --gres=gpu:${gpu_type}:${n_gpus} scripts/scratch/en_char_uz_char_til.sh"
      flags:
        experiment_name:
          required: true
        model_name:
          required: true
        references_file:
          required: true
        raw_data_folder:
          required: true
        bin_data_folder:
          required: true
        conda_env_name:
          default: "fairseq-py3.8"
        experiments_folder:
          default: "$HOME/mrl_nmt22/experiments"
        checkpoints_folder:
          default: "$HOME/mrl_nmt22/checkpoints"
        gpu_type:
          default: "V100"
          choices:  # Supported on Brandeis HPCC
            - V100
            - RTX2
            - GTX
            - TitanX
            - TitanXP
            - TeslaM
        n_gpus:
          type: int
          default: 5
      env:
        MRL_NMT_EXPERIMENT_NAME: "${experiment_name}"
        MRL_NMT_MODEL_NAME: "${model_name}"
        MRL_NMT_REFERENCES_FILE: "${references_file}"
        MRL_NMT_RAW_DATA_FOLDER: "${raw_data_folder}"
        MRL_NMT_BIN_DATA_FOLDER: "${bin_data_folder}"
        MRL_NMT_ENV_NAME: "${conda_env_name}"
        MRL_NMT_EXPERIMENTS_FOLDER: "${experiments_folder}"
        MRL_NMT_CHECKPOINTS_FOLDER: "${checkpoints_folder}"
      requires:
        - file: scripts
        - file: config
        - file: data-bin
        - file: checkpoints
        - file: experiments
    preprocess_yaml_config:
      description: "Submit a SLURM job to process a dataset per a YAML config"
      exec: "sbatch scripts/scratch/slurm_preprocess_yaml_config.sh"
      flags:
        yaml_config:
          required: true
        conda_env_name:
          default: "fairseq-py3.8"
      env:
        MRL_NMT_YAML_CONFIG: "${yaml_config}"
        MRL_NMT_ENV_NAME: "${conda_env_name}"
      requires:
        - file: scripts
        - file: config
    echo_vars:
      description: "Submit a SLURM job to echo out some variable values"
      exec: "sbatch scripts/scratch/echo_vars.sh"
      flags:
        a:
          required: true
        b:
          required: true
      env:
        TEST_ASDF: "${a}"
        TEST_FDSA: "${b}"
      requires:
        - file: scripts

# Transformer ops
- model: nmt
  operations:
    remove_experiment:
      description: "Remove an experiment by deleting its corresponding folder in experiments/ and checkpoints/"
      exec: "bash scripts/remove_experiment.sh ${experiment_name}"
      flags:
        experiment_name:
          type: string
          required: true
      requires:
        - file: checkpoints
        - file: experiments
        - file: scripts
    prep_experiment:
      description: "Prepares an experiment folder for an MT experiment"
      exec: "bash scripts/prep_experiment.sh ${experiment_name} ${raw_data_folder} ${bin_data_folder} ${model_name}"
      flags:
        $include: ["basic-flags", "prep-flags"]
      requires:
        - file: data-bin
        - file: checkpoints
        - file: experiments
        - file: scripts
    train_transformer:
      description: "Train transformer model"
      exec: "bash scripts/train_transformer ${seed} ${criterion} ${label_smoothing} ${optimizer} ${lr} ${lr_scheduler} ${warmup_init_lr} ${warmup_updates} ${clip_norm} ${max_updates} ${save_interval_updates} ${encoder_layers} ${encoder_attention_heads} ${decoder_layers} ${decoder_attention_heads} ${activation_fn} ${batch_size} ${p_dropout} ${decoder_embedding_dim} ${decoder_hidden_size} ${encoder_embedding_dim} ${encoder_hidden_size} ${experiment_name} ${gpu_device} ${validate_interval} ${validate_interval_updates} ${src_lang} ${tgt_lang} ${max_epoch} ${model_name} ${max_tokens} ${remove_preprocessing_source} ${remove_preprocessing_hypotheses} ${remove_preprocessing_references} ${eval_bleu_detok}"
      flags:
        $include: ["basic-flags", "transformer-flags"]
        remove_preprocessing_source:
          type: string
          default: "none"
        remove_preprocessing_hypotheses:
          type: string
          default: "none"
        remove_preprocessing_references:
          type: string
          default: "none"
        eval_bleu_detok:
          type: string
          default: "space"
          choices:
            - moses
            - spm
            - space
      requires:
        - file: data-bin
        - file: scripts
        - file: checkpoints
        - file: experiments
    evaluate_transformer:
      description: "Evaluate transformer model"
      exec: "bash scripts/evaluate_transformer ${experiment_name} ${mode} ${beam_size} ${seed} ${gpu_device} ${model_name} ${src_lang} ${tgt_lang} ${eval_name} ${remove_preprocessing_source} ${remove_preprocessing_hypotheses} ${remove_preprocessing_references} ${remove_preprocessing_references_clean} ${detokenize_source} ${detokenize_hypotheses} ${detokenize_references} ${detokenize_references_clean} ${references_clean_file} ${use_cpu_for_eval}"
      flags:
        $include: ["basic-flags", "evaluate-flags"]
        eval_name:
          type: string
          default: "eval_${model_name}"
      output-scalars:
        - 'EVAL_SCALAR: (\key)\t(\value)'
      requires:
        - file: data-bin
        - file: scripts
        - file: checkpoints
        - file: experiments
    experiment_pipeline:
      description: "Prep experiment -> train model -> evaluate model"
      flags:
          $include: ["basic-flags", "prep-flags", "transformer-flags", "evaluate-flags"]
      steps:
          - run: prep_experiment
            flags:
                experiment_name: "${experiment_name}"
                src_lang: "${src_lang}"
                tgt_lang: "${tgt_lang}"
                model_name: "${model_name}"
                seed: "${seed}"
                gpu_device: "${gpu_device}"
                raw_data_folder: "${raw_data_folder}"
                bin_data_folder: "${bin_data_folder}"
          - run: train_transformer
            flags:
                experiment_name: "${experiment_name}"
                src_lang: "${src_lang}"
                tgt_lang: "${tgt_lang}"
                model_name: "${model_name}"
                seed: "${seed}"
                gpu_device: "${gpu_device}"
                criterion: "${criterion}"
                label_smoothing: "${label_smoothing}"
                lr: "${lr}"
                lr_scheduler: "${lr_scheduler}"
                warmup_init_lr: "${warmup_init_lr}"
                clip_norm: "${clip_norm}"
                save_interval_updates: "${save_interval_updates}"
                activation_fn: "${activation_fn}"
                encoder_layers: "${encoder_layers}"
                decoder_layers: "${decoder_layers}"
                encoder_attention_heads: "${encoder_attention_heads}"
                decoder_attention_heads: "${decoder_attention_heads}"
                decoder_embedding_dim: "${decoder_embedding_dim}"
                decoder_hidden_size: "${decoder_hidden_size}"
                encoder_embedding_dim: "${encoder_embedding_dim}"
                encoder_hidden_size: "${encoder_hidden_size}"
                batch_size: "${batch_size}"
                max_tokens: "${max_tokens}"
                optimizer: "${optimizer}"
                p_dropout: "${p_dropout}"
                max_updates: "${max_updates}"
                max_epoch: "${max_epoch}"
                validate_interval: "${validate_interval}"
                validate_interval_updates: "${validate_interval_updates}"
                warmup_updates: "${warmup_updates}"
          - run: evaluate_transformer
            flags:
                experiment_name: "${experiment_name}"
                src_lang: "${src_lang}"
                tgt_lang: "${tgt_lang}"
                model_name: "${model_name}"
                seed: "${seed}"
                gpu_device: "${gpu_device}"
                mode: "${mode}"
                beam_size: "${beam_size}"
                remove_preprocessing_source: "${remove_preprocessing_source}"
                remove_preprocessing_hypotheses: "${remove_preprocessing_hypotheses}"
                remove_preprocessing_references: "${remove_preprocessing_references}"
                remove_preprocessing_references_clean: "${remove_preprocessing_references_clean}"
                detokenize_source: "${detokenize_source}"
                detokenize_hypotheses: "${detokenize_hypotheses}"
                detokenize_references: "${detokenize_references}"
                detokenize_references_clean: "${detokenize_references_clean}"
                references_clean_file: "${references_clean_file}"
                use_cpu_for_eval: "${use_cpu_for_eval}"
      requires:
        - file: data-bin
        - file: scripts
        - file: checkpoints
        - file: experiments
